{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a0f628",
   "metadata": {},
   "source": [
    "# 2) Scraping data\n",
    "\n",
    "## Retrieving information about the website\n",
    "\n",
    "Based on the files from the `/data/raw` directory (generated in exercise 1) extract the following information about an offer:\n",
    "\n",
    "- location - both city and country. For remote work, set `Remote` as the city and `N/A` as the country,\n",
    "- salary - both lower and upper limits and currency. If there is no pay range, write the same value in both fields (lower limit = upper limit),\n",
    "- name of position,\n",
    "- company,\n",
    "- technology.\n",
    "\n",
    "Write the results of a single bid into a dictionary with the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    'name': 'name of the position',\n",
    "    'company': 'name of the employer',\n",
    "    'technology': 'name of the used technology',\n",
    "    'job': 'information regarding name of the search e.g. data analyst ',\n",
    "    'location': {'city': 'city of employment', 'country': 'country of employment'},\n",
    "    'salary': {'low': 'lower limit', 'high': 'higher limit', 'currency': 'salary currency'} \n",
    "}\n",
    "``` \n",
    "\n",
    "Put single items into a list.\n",
    "\n",
    "A list of such dictionaries can be read using another `Pandas` method - `json_normalize`. It is shown during the workshop, because json is a commonly used construct for communication between modules.\n",
    "\n",
    "Save the results as `DataFrame` to `data\\interim\\job_offers.csv` using the `;` separator, `UTF-8` encoding, and without index (`index=False`).\n",
    "\n",
    "Complete the exercise following the steps:\n",
    "\n",
    "- Write a function that takes the HTML code of a page and returns a list with pieces of HTML code that contain information about a single ad,\n",
    "- Write a function that will take the HTML code containing information about one ad and return a dictionary with the information (described above), 3 Assemble this into a working script that:\n",
    "    - Finds all files in the data\\raw directory,\n",
    "    - For each file:\n",
    "        - Divides it into sections corresponding to the company,\n",
    "        - Extracts the necessary information from it as a dictionary,\n",
    "        - Will add the dictionary to the previously created list,\n",
    "    - Loads the list with dictionaries using Pandas into the dataset,\n",
    "    - Saves the dataset in the data\\interim\\ directory with the current date.\n",
    "\n",
    "#### File names\n",
    "\n",
    "We will adopt the following file naming convention:\n",
    "\n",
    "```\n",
    "'job_offers_{current date}.csv'\n",
    "```\n",
    "\n",
    "Where the `{current date}` parameter should use the `yyyy_mm_dd` format (year month day).\n",
    "\n",
    "#### Hints\n",
    "\n",
    "- To get the current date you can use the code: `datetime.today().strftime('%Y_%m_%d')`. Remember to import the appropriate module!\n",
    "- You can split the data parsing for a single offer into several smaller helper functions. For example, one can retrieve the salary, another - parse the location data. This will make the code easier to maintain.\n",
    "- To test the performance of your functions, you can manually pull HTML code from a file and pass it as a parameter. This way you don't need the whole script to test how its parts work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b223a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import glob \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907945f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"data/raw/*.html\"  #regex for finding all html files in folder raw\n",
    "\n",
    "files = glob.glob(file_path)  #all html files in folder raw\n",
    "\n",
    "ads_information_complete = pd.DataFrame()  #this will be the final dataframe\n",
    "\n",
    "for file in files:   #for each html file\n",
    "    \n",
    "    regex = r\"\\\\\\w*\\s\\w*_\"  #regex for indicating job_name from the file name\n",
    "    job_name = re.search(regex,file).group().replace(\"\\\\\",\"\").replace(\"_\",\"\")\n",
    "    \n",
    "    ads_information = []  #list of information of an offers (dictionaries) that are included on the page\n",
    "\n",
    "    with open(file, \"r\", encoding = \"utf-8\") as file:\n",
    "        html_content = file.read()  #reading the html code\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")  \n",
    "    jobs = soup.find_all(\"aside\", class_ = \"tw-w-full\")   #here is information about all offers on the page\n",
    "\n",
    "    for job in jobs:  #for each offer\n",
    "        \n",
    "        #city\n",
    "        city = job.find(\"span\", \n",
    "                        class_ = \"tw-text-ellipsis tw-inline-block tw-overflow-hidden tw-whitespace-nowrap tw-max-w-[100px] md:tw-max-w-[200px] tw-text-right\").text.strip()\n",
    "        \n",
    "        #country\n",
    "        country = job.find(\"div\", \"tw-flex tw-items-center ng-star-inserted\").contents[1].text.replace(\",\",\" \").strip()\n",
    "        if country == \"\":  #if missing\n",
    "            country = \"N/A\"\n",
    "\n",
    "        #salary (range) - object\n",
    "        salary_obj = job.find(\"span\",\n",
    "                            class_ = \"text-truncate badgy salary lg:tw-btn tw-text-ink lg:tw-btn-secondary-outline tw-text-xs lg:tw-py-0.5 lg:tw-px-2 ng-star-inserted\")\n",
    "\n",
    "        #lower salary threshold\n",
    "        if salary_obj != None:   #if data about salary does exist\n",
    "            salary = salary_obj.text #salary range\n",
    "            try:  #is it range?\n",
    "                regex = r\"\\d+\\s\\d+\\s*–\"\n",
    "                low = re.search(regex,salary).group().replace(\"–\",\"\") \n",
    "                low_int = int(re.sub(\"\\s\",\"\", low))  #remove space in the number and format as integer\n",
    "            except AttributeError:  #not range\n",
    "                regex = r\"\\d+\\s\\d+\"  #to retrieve the number\n",
    "                low = re.search(regex,salary).group()\n",
    "                low_int = int(re.sub(\"\\s\",\"\", low))\n",
    "\n",
    "        else:\n",
    "            low_int = None\n",
    "\n",
    "        #upper salary threshold\n",
    "        if salary_obj != None: \n",
    "            salary = salary_obj.text\n",
    "            try:  #is it range? \n",
    "                regex = r\"–\\s*\\d+\\s\\d+\"\n",
    "                up = re.search(regex,salary).group().replace(\"–\",\"\") \n",
    "                up_int = int(re.sub(\"\\s\",\"\", up)) \n",
    "            except AttributeError:  \n",
    "                up_int = low_int \n",
    "        else:\n",
    "            up_int = None\n",
    "\n",
    "        #currency\n",
    "        if salary_obj != None: \n",
    "            salary = salary_obj.text\n",
    "            regex = r\"[A-Z]+\"   #to retrieve the currency\n",
    "            curr = re.search(regex,salary).group()\n",
    "        else:\n",
    "            curr = None\n",
    "\n",
    "        #position name\n",
    "        position = job.find(\"h3\").text.strip()\n",
    "\n",
    "        #company name\n",
    "        company = job.find(\"h4\").text.strip()\n",
    "\n",
    "        #technology\n",
    "        techs0 = job.find_all(\"span\", class_ = \"lg:tw-text-gray-60 lg:tw-border-2 lg:tw-border-gray-ddd tw-text-xs lg:tw-py-0.5 lg:tw-px-2 tw-text-gray-60\")\n",
    "        techs = [tech.text.strip() for tech in techs0]  #create a list of technologies\n",
    "\n",
    "        #write the information into a dictionary \n",
    "        information = {\n",
    "        'name': position,\n",
    "        'company': company,\n",
    "        'technology': techs,\n",
    "        'job': job_name,\n",
    "        'location': {'city': city, 'country': country},\n",
    "        'salary': {'low': low_int, 'high': up_int, 'currency': curr} \n",
    "        }\n",
    "\n",
    "        ads_information.append(information)  #append it to the list of dictionaries\n",
    "        \n",
    "    ads_information = pd.DataFrame(ads_information)  #transform to a dataframe\n",
    "    \n",
    "    #append each dataframe to the previous one\n",
    "    ads_information_complete = pd.concat([ads_information_complete, ads_information], axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99612879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of an offers before removing duplicates: 1169.\n",
      "Total number of an offers after removing duplicates: 326.\n"
     ]
    }
   ],
   "source": [
    "#remove duplicate rows based on the same name of the position, company, job\n",
    "#(note: pages are not separate, but each subsequent page contains the previous ones)\n",
    "\n",
    "ads_information_complete2 = ads_information_complete.drop_duplicates(subset=[\"name\", \"company\", \"job\"])\n",
    "\n",
    "print(f\"Total number of an offers before removing duplicates: {len(ads_information_complete)}.\")\n",
    "print(f\"Total number of an offers after removing duplicates: {len(ads_information_complete2)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631322c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of an offers after removing all duplicates: 150.\n"
     ]
    }
   ],
   "source": [
    "#dataframe ads_information_complete2 contains other kind of duplicates - some offers have appeared for more job positions\n",
    "#we need to remove these duplicates and match the offer to the job by name\n",
    "\n",
    "#all duplicates\n",
    "duplicates = ads_information_complete2[ads_information_complete2.duplicated(subset=[\"name\", \"company\"], keep = False)]\n",
    "\n",
    "\n",
    "for index, row in duplicates.iterrows():\n",
    "    if re.search(r\"analyst\", row[\"name\"].lower()):\n",
    "        row[\"job\"] = \"data analyst\"\n",
    "    elif re.search(r\"engineer\", row[\"name\"].lower()):\n",
    "        row[\"job\"] = \"data engineer\"\n",
    "    elif re.search(r\"scientist\", row[\"name\"].lower()):\n",
    "        row[\"job\"] = \"data scientist\"\n",
    "\n",
    "#finally delete these duplicates through the columns name, company, job\n",
    "ads_information_complete3 = duplicates.drop_duplicates(subset = [\"name\", \"company\", \"job\"])\n",
    "\n",
    "#note: we will keep other duplicates that cannot be assigned to these positions\n",
    "\n",
    "print(f\"Total number of an offers after removing all duplicates: {len(ads_information_complete3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee65d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv with current date\n",
    "\n",
    "current_date = datetime.datetime.today().strftime(\"%Y_%m_%d\")\n",
    "file_name = f\"job_offers_{current_date}.csv\"\n",
    "    \n",
    "ads_information_complete3.to_csv(r\"data\\interim\\{}\".format(file_name), sep = \";\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
